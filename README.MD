## Fake News Detector

Authors: Mngomezulu Mkhenso, Chauke Ntshovelo Brilliant

## Overview / Objective

Build a classifier that distinguishes fake vs real news articles.

## Deliverables

Cleaned dataset and preprocessing pipeline (stemmed_dataset.csv), NOTE THAT THIS FILE IS NOT INCLUDED WITHIN THE REPO BUT IT CAN EASILY BE GENERATED BY RUNNIGN THE CODES.

## Baseline model: Logistic Regression

Evaluation: confusion matrix, ROC, PR curve, learning curves

Saved artifacts for deployment: tfidf_vectorizer.pkl, fake_news_detector.pkl

## Dataset (source & composition)
 https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?utm_source=chatgpt.com

## Datasets used: 
Fake.csv and True.csv â†’ combined into new_dataset.csv

Combined dataset summary (from the training run):

Total samples: 44898
Class distribution:
label
1    0.523
0    0.477
Name: proportion, dtype: float64


# Fake News Detector â€” Sample Records

A short example showing the first rows from the dataset used for training/testing the fake-news detector.  
The table shows the `title`, `text`, `subject`, and `label` columns. Labels are `0` (real) and `1` (fake) â€” adjust if your project uses a different convention.

---

## Sample records (first rows)

<!-- HTML table for nicer formatting in GitHub README -->
<table>
  <thead>
    <tr>
      <th style="text-align:left">index</th>
      <th style="text-align:left">title</th>
      <th style="text-align:left">text (truncated)</th>
      <th style="text-align:left">subject</th>
      <th style="text-align:center">label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>As U.S. budget fight looms, Republicans flip t...</td>
      <td>WASHINGTON (Reuters) - The head of a conservative...</td>
      <td>politicsNews</td>
      <td style="text-align:center">0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>U.S. military to accept transgender recruits o...</td>
      <td>WASHINGTON (Reuters) - Transgender people will...</td>
      <td>politicsNews</td>
      <td style="text-align:center">0</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Senior U.S. Republican senator: 'Let Mr. Muell...</td>
      <td>WASHINGTON (Reuters) - The special counsel inv...</td>
      <td>politicsNews</td>
      <td style="text-align:center">0</td>
    </tr>
    <tr>
      <td>3</td>
      <td>FBI Russia probe helped by Australian diplomat...</td>
      <td>WASHINGTON (Reuters) - Trump campaign adviser ...</td>
      <td>politicsNews</td>
      <td style="text-align:center">0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Trump wants Postal Service to charge 'much mor...</td>
      <td>SEATTLE/WASHINGTON (Reuters) - President Donal...</td>
      <td>politicsNews</td>
      <td style="text-align:center">0</td>
    </tr>
  </tbody>
</table>

---

## Column descriptions

- **title** â€” Short headline for the news article.  
- **text** â€” Article body content (often truncated in displays).  
- **subject** â€” Topic label or category (e.g., `politicsNews`).  
- **label** â€” Ground-truth target used for supervised learning:  
  - `0` = Real / True news  
  - `1` = Fake news

---

Notes: dataset is roughly balanced in this run (~52.3% fake, 47.7% real).

Preprocessing pipeline (what was applied & why)

Inputs combined â†’ new feature content: title + text + subject

Pipeline applied to content:

Concatenate fields â€” more signal (title often contains high-signal tokens).

Lowercase & remove non-letters â€” normalize tokens and remove punctuation/noise.

Stopword removal (NLTK stopwords) â€” removes high-frequency low-information tokens.

Stemming (PorterStemmer) â€” reduces inflectional variants to stems to lower sparsity.

Save stemmed dataset â†’ stemmed_dataset.csv (reproducibility).

Reasoning: Classic NLP preprocessing reduces vocabulary size and noise, improving TF-IDF signal and making linear model weights more meaningful.

Example stopwords (printed in run):

['a', 'about', 'above', 'after', 'again', 'yours', 'yourself', 'yourselves', "you've", ...]


Example of stemmed content (first rows):

0    u budget fight loom republican flip fiscal scr...
1    u militari accept transgend recruit monday pen...
2    senior u republican senat let mr mueller job w...
3    fbi russia probe help australian diplomat tip ...
4    trump want postal servic charg much amazon shi...
...
44893 mcpain john mccain furiou iran treat us sailor...
44894 justic yahoo settl e mail privaci class action...

Feature engineering â€” TF-IDF

Used TfidfVectorizer() on the preprocessed content.

Why TF-IDF?

Captures importance of terms relative to corpus frequency (downweights ubiquitous words).

Produces a sparse numeric matrix suited to linear models (fast & memory-efficient).

Effective strong baseline for text classification.

Artifact saved: tfidf_vectorizer.pkl
(Printout from run: Vectorizer saved!)

Train / test split & sampling

Split used:

train_test_split(..., test_size=0.2, stratify=news_dataset['label'], random_state=2)


Why stratify? preserves class distribution â†’ stable metrics between train & test.

Why random_state? reproducible experiments.

Model choice â€” Logistic Regression

Model: LogisticRegression() (scikit-learn defaults)

Why Logistic Regression?

Interpretability: coefficients show term impact direction.

Efficiency: scales well for high-dimensional sparse TF-IDF features.

Strong baseline: quick to train and easy to regularize; commonly used as first pass for text tasks.

Training & saved artifacts

Training call:

model.fit(X_train, Y_train)


Saved artifacts

tfidf_vectorizer.pkl â€” TF-IDF vectorizer (joblib)

fake_news_detector.pkl â€” trained model (joblib)

Run printouts (examples from your run):

Vectorizer saved!
Trained model saved successfully as fake_news_detector.pkl! Ready for deployment.

Evaluation â€” what was computed & why

Metrics computed in code:

Accuracy (train & test)

Precision, recall, F1 (classification report)

Confusion matrix (heatmap)

ROC-AUC and ROC curve

Precision-Recall curve + Average Precision

Learning curves (diagnose bias/variance)

Distribution of predicted probabilities

Why these metrics?

Accuracy: overall correctness

Precision / Recall / F1: class-wise performance (important for fake-news trade-offs)

ROC & PR: threshold-independent views; PR is especially informative for class imbalance

Learning curves: reveal under/overfitting patterns

Predicted-prob distributions: show model calibration / separability

Placeholders: the code prints precise numeric metrics at runtime (accuracy, ROC-AUC, Average Precision). Replace the placeholders below with your run numbers when assembling slides.

# ðŸ§  Model Evaluation Results

Below are the key performance visualizations for the Fake News Detector model.  
Each plot highlights different aspects of the modelâ€™s predictive performance.

---

## ðŸ“Š ROC-AUC Score
Shows the trade-off between the True Positive Rate and False Positive Rate.  
A higher area under the curve (AUC) indicates stronger model discrimination ability.

<p align="center">
  <img src="./evaluation/ROC_AUC_SCORE.png" alt="ROC-AUC Score" width="70%">
</p>

---

## ðŸŽ¯ Average Precision Score (AP)
Displays the relationship between precision and recall, summarizing model performance for imbalanced datasets.

<p align="center">
  <img src="./evaluation/PRECISION_SCORE.png" alt="Average Precision Score" width="70%">
</p>

---

## ðŸ“ˆ Learning Curves
Visualizes model performance over training epochs.  
It helps diagnose **underfitting** or **overfitting** by comparing training vs. validation accuracy/loss.

<p align="center">
  <img src="./evaluation/LEARNING_CURVES.png" alt="Learning Curves" width="70%">
</p>

---

## ðŸ“‰ Distribution of Predicting Probabilities by True Class
Shows how confidently the model predicts probabilities for each class,  
helping assess calibration and separation between true and false samples.

<p align="center">
  <img src="./evaluation/Distribution_of_predicting_probabilities_by_true_class.png" alt="Distribution of Predicting Probabilities by True Class" width="70%">
</p>

---

## ðŸ§© Confusion Matrix
Illustrates the number of **true positives, true negatives, false positives, and false negatives**.  
A good model will show a strong diagonal (high correct classifications).

<p align="center">
  <img src="./evaluation/CONFUSION_MATRIX.png" alt="Confusion Matrix" width="70%">
</p>

---

> ðŸ’¡ **Tip:** All evaluation plots are located in the `evaluation/` directory.  
> You can regenerate them using your training script (`training.py`) or evaluation notebook.



Interpretation &  reading of results

Confusion matrix: choose operating point based on whether recall (catching fake) or precision (avoiding false alarms) is prioritized.

ROC vs PR: For imbalanced data PR and AP are often more informative for the positive (fake) class.

Learning curves:

If train â‰« val â†’ overfitting (add regularization, collect more data, reduce model complexity).

If both low & close â†’ underfitting WE then use richer features or a stronger model.

Calibration: If probabilities are poorly separated/calibrated, use Platt scaling or isotonic calibration.

Limitations & ethical considerations

Limitations

Dataset bias : site-specific wording and temporal drift.

Model may learn publication/source artifacts rather than true semantic signals.

Simple preprocessing (stemming) can remove useful nuance.

Ethics

False positives can hurt reputations; false negatives reduce usefulness.

Use human-in-the-loop for decisions with high harm potential.

Be transparent about confidence and model limitations.

Next steps & improvements

Try contextual embeddings (BERT/fine-tuning) to capture semantics.

Calibrate probability outputs for decision thresholds.

Add explainability (LIME/SHAP) for per-sample rationale.

Monitor in production for drift; implement retraining triggers.

Consider ensembling or transformer-based fine-tuning for improved performance.

Deployment plan (high level)

Bundle tfidf_vectorizer.pkl + fake_news_detector.pkl in service.

API workflow:

Client sends article â†’ preprocessing â†’ vectorize â†’ model.predict_proba â†’ return probability + explanation tokens.


How to reproduce

Create virtual env, install requirements (example):

python -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows
pip install -r requirements.txt

Run the notebook in ML-Model.py. this will produce new_dataset.csv and stemmed_dataset.csv and models. The models are also included you can safely delete them and rebuild/regenerate using the notebook.ipynb.

To expose the trained models via the api.
while your virtual env is still activated. change directory to backend folder and open the backend folder on the intergrated terminal (vs code).

the backend consists of two files:
            preprocess.py which preprocesses the inputs from the front-end and main.py which is the actual FastAPI that you can run on the terminal using python main.py

after that you can open the front-end folder in intergrated terminal (make sure your virtual enviroment is active in this terminal.)

there is app.py which is a streamlit app that test the code using the api . there is also the app folder which is a react + vite front-end for testing the aplication which you can also access using https://fake-news-detector-ef6d7.web.app/ on the browser.

note that the front end is wired to the hosted API so no need to run it.


Summary of the notebook
Loads Fake.csv + True.csv

Concatenates into new_dataset.csv

Creates content, applies stemming & stopword removal

Fits TfidfVectorizer â†’ tfidf_vectorizer.pkl

Fits LogisticRegression â†’ fake_news_detector.pkl

Produces plots (save with plt.savefig(...))

Appendix â€” Run outputs. You should see something like :
Total samples: 44898
Class distribution:
label
1    0.523
0    0.477
Name: proportion, dtype: float64

Sample records: (title/text/subject/label printed earlier)

Vectorizer saved!
... (plots generated by code)
Trained model saved successfully as fake_news_detector.pkl! Ready for deployment.

artifacts
Notebook: (provide filename or link)
Artifacts: tfidf_vectorizer.pkl, fake_news_detector.pkl, stemmed_dataset.csv
